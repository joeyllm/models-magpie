# ===============================
# Mistral 7B Fine-Tuning Config
# ===============================

defaults:
  - _self_

# -------------------------------
# WandB Config
# -------------------------------
wandbconfig:
  project: Mistral7B_AU
  name: qlora-bf16
  mode: offline   # "online", "offline", or "disabled"

# -------------------------------
# Dataset Config
# -------------------------------
dataconfig:
  data_path: data/au_fineweb_00000.parquet # to be edited
  batch_size: 16
  num_workers: 4
  shuffle: false
  drop_last: true
  pin_memory: true
  max_seq_len: 4096

# -------------------------------
# Model Config
# -------------------------------
modelconfig:
  name: mistralai/Mistral-7B-v0.1
  load_in_4bit: true # Enable 4-bit quantization
  lora_r: 8 
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# -------------------------------
# Optimizer Config
# -------------------------------
optimizerconfig:
  lr: 2e-4
  betas: [0.9, 0.95]
  weight_decay: 0.01

# -------------------------------
# Trainer Config
# -------------------------------
trainconfig:
  epochs: 1
  total_steps: 0            # 0 -> let Trainer compute from dataset
  accumulation_steps: 16
  output_dir: checkpoints/mistral7b-au
  logging_steps: 10
  save_steps: 500
  save_total_limit: 2
  gradient_checkpointing: true
  bf16: true
  seed: 42
  packing: true             # pack multiple examples per sequence

# -------------------------------
# Scheduler Config
# -------------------------------
schedulerconfig:
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  max_grad_norm: 0.3