# configs/config.yaml
# ===============================
# Mistral 7B Fine-Tuning Config
# ===============================

defaults:
  - _self_

# -------------------------------
# WandB Config
# -------------------------------
wandbconfig:
  project: Magpie-Test-01
  entity: JoeyLLM
  name: lora-bf16-fsdp-test
  mode: disabled   # "online", "offline", or "disabled"

# -------------------------------
# Dataset Config
# -------------------------------
dataconfig:
  data_path: outputs/data/ # Contains folders that have the parquet files
  batch_size: 2
  num_workers: 8
  shuffle: false
  drop_last: true
  pin_memory: true
  max_seq_len: 1024

# -------------------------------
# Model Config
# -------------------------------
modelconfig:
  name: mistralai/Mistral-7B-v0.3
  load_in_4bit: true 
  lora_r: 8 
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# -------------------------------
# Optimizer Config
# -------------------------------
optimizerconfig:
  lr: 5e-5
  betas: [0.9, 0.95]
  weight_decay: 0.01

# -------------------------------
# Trainer Config
# -------------------------------
trainconfig:
  epochs: 1
  total_steps: 0            # 0 -> let Trainer compute from dataset
  accumulation_steps: 8
  output_dir: checkpoints/mistral7b-au
  logging_steps: 10
  save_steps: 2000
  save_total_limit: 2
  gradient_checkpointing: true
  bf16: true
  seed: 42
  packing: true             # pack multiple examples per sequence

# -------------------------------
# Scheduler Config
# -------------------------------
schedulerconfig:
  lr_scheduler_type: cosine
  warmup_ratio: 0.2
  max_grad_norm: 0.3