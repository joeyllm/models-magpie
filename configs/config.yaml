# ===============================
# Mistral 7B Fine-Tuning Config
# ===============================

defaults:
  - _self_

# -------------------------------
# WandB Config
# -------------------------------
wandbconfig:
  project: MistralAU
  name: mistral-lora-au
  mode: offline  # "online", "offline", or "disabled"

# -------------------------------
# Dataset Config
# -------------------------------
dataconfig:
  data_path: data/au_fineweb_00000.parquet # to be edited
  batch_size: 16
  num_workers: 4
  shuffle: false
  drop_last: true
  pin_memory: true

# -------------------------------
# Model Config
# -------------------------------
modelconfig:
  name: mistralai/Mistral-7B-v0.1
  load_in_4bit: false # Enable 4-bit quantization
  lora_r: 8 
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# -------------------------------
# Optimizer Config
# -------------------------------
optimizerconfig:
  lr: 2e-4
  betas: [0.9, 0.95]
  weight_decay: 0.01

# -------------------------------
# Trainer Config
# -------------------------------
trainconfig:
  total_steps: 5000
  epochs: 10
  accumulation_steps: 4
  save_model_path: model/mistral_lora.pt
  log_freq: 50

# -------------------------------
# Scheduler Config
# -------------------------------
schedulerconfig:
  max_lr: 2e-4
  pct_start: 0.01
  anneal_strategy: cos
  div_factor: 25.0
  final_div_factor: 10000.0
  cycle_momentum: true
  base_momentum: 0.85
  max_momentum: 0.95
  three_phase: false